The statistics and index files for the October 2019 crawl archive can be found at https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2019-43/index.html
The two links warc.paths.gz and wet.paths.gz on this page lead to the download locations for the index files of the WARC and WET records.
Each index file contains 56000 lines and each line represents a URL fragment that points to a gzipped file which contains a subset of the entire crawl, so the entire crawl content for October is spread across 56000 files.
To obtain a valid URL, the lines need to be prefixed by https://commoncrawl.s3.amazonaws.com/ or s3://commoncrawl/ .
For example, the full download paths for the first WARC and WET files from the first line of the corresponding index/path files would be
https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2019-43/segments/1570986647517.11/warc/CC-MAIN-20191013195541-20191013222541-00000.warc.gz
https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2019-43/segments/1570986647517.11/wet/CC-MAIN-20191013195541-20191013222541-00000.warc.wet.gz


To parse these files in Python, the source has to be transformed to a CSV file via scala.packt2.spark.Convert.scala
and can then be read into a PySpark DataFrame as in python.packt2.spark.read_webcrawl.py
I'm currently working on a better solution that should work without this intermediate step