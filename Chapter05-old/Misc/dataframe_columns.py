spark = SparkSession \
   .builder \
   .appName("dataframe_columns") \
   .getOrCreate()

df.columns

df.columns[2]
df.columns[2:5]

df.dtypes

df.select(df.id).take(5)

string_id_df = df.select(df.id.cast("string"))
string_id_df.take(5)

string_id_df.schema

df.select(df.id.cast("string").alias('new_id')).show(5)

# Drop Columns

df.drop("ip_address")
df.drop("comments", "registration_dttm")

df.drop(df.first_name)
df.drop(df.first_name).drop(df.title)

# Rename Columns

print(df.columns)
df_renamed = df.withColumnRenamed("title", "role")
print(df_renamed.columns)


from pyspark.sql.functions import lit
server_df = df.withColumn("constant_string", lit("generated by server")).select("id", "constant_string")
server_df.show(5)
server_df.printSchema()
